## 1. 说说小文件产生的原因，小文件的危害，如何处理小文件
### 原因：
1. insert into一次，就会产生一个文件
2. load data加载数据产生文件
3. 动态分区产生小文件：**文件数量=ReduceTask数量X分区数**
### 危害：
1. 对namenode的影响：⼤量的元数据占⽤⼤量namenode的内存
2. 对datanode的影响：花费在寻址上的时间要比较长
3. 对计算性能的影响：⼤量的⼩⽂件，会启动更多的map任务调度
### 处理：
set mapred.max.split.size=112345600;       --最大切片大小
set mapred.min.split.size.per.node=112345600;--每个节点最小切片大小
set mapred.min.split.size.per.rack=112345600; --每个机架的最小切片大小
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
#### --小文件合并(方法1)
insert overwrite table 表 select * from emp;--假设emp有很多小文集(方法2)
alter  table  表名  concatenate  --对orc表格小文件合并(方法3)
## 2. 列举出你在信贷项目中出现的5个数据倾斜出现的场景， 你是怎么发现数据倾斜的，最后是怎么解决的
**Data Skew Scenarios in Credit Projects:**  
1. **用户ID分布不均**：部分用户ID数据量远大于其他；
2. **时间戳分布异常**：某些时段的交易记录异常集中；
3. **账户编号带有前缀**：某些账户编号集中于特定范围导致数据分布不均；
4. **特征字段（如地区、产品类型）不均匀**：部分标签的记录数量远大于其他；
5. **数据清洗逻辑错误**：如ID未进行去重或映射错误，导致聚合时出现异常。
**How to detect skew:**  
- 通过Hive SQL执行`EXPLAIN`查看执行计划，观察Reduction阶段的数据分布；
- 查看MapReduce任务的输出文件大小，发现某些Reducer的处理数据量过大；
- 使用`Hive DataFrame`或`Spark SQL`中的`count`和`percentile`分析字段分布；
- 使用`Hive CLI`或`Spark UI`观察任务运行时的性能瓶颈；
- 在数据写入后，对特定维度进行抽样分析，识别数据分布异常。

**How to resolve skew:**  
- **采用随机前缀（salt）方法**：在原始ID上添加随机数进行重新分区；
- **使用Hive的`skewjoin`**：在Join操作中优化数据分布；
- **优化数据写入键值**：优先选择分布更均匀的字段作为分区字段（如`hash`或`partition by`字段）；
- **使用Flink或Spark的`repartition`或`coalesce`**：重新划分数据分区；
- **将数据进行分桶处理（Bucketing）**，使数据更均匀地分布到多个文件中；
- **在ETL过程中进行数据预处理**，如过滤异常值、分组聚合、均衡分布等；
- **在数据源端调整采集方式**，使数据按更均匀的方式写入；
## 3. 列举出你在监管报送或者用户画像项目中是如何优化hive sql的
1. hive存储压缩优化，比如使用parquet + lzo 或 orc + snappy
2. 表连接的时候，使用小表join大表 (因为可以减少磁盘的读取次数)
3. 表连接或者汇总计算的时候避免数据倾斜:
   a.表连接可以使用map join 避免数据倾斜
   b.汇总计算group by 可以使用负载均衡
	或者把倾斜的数据单独拿出来计算
   c.当空值很多的时候可以过滤空值或者通过添加随机数改变空值
4. 用GROUP BY + COUNT 代替 Count(Distinct) 去重统计
5. 避免使用笛卡尔积
6. 行列裁剪：先where筛选，再进行连接计算; select只选择需要的字段
7. 合理使用分区分桶表
8. 合理设置map数：
	当小文件过多的时候，可以通过合并小文件减少map数量
	<span style="background:rgba(173, 239, 239, 0.55)">set mapred.max.split.size=112345600;</span>         
	<span style="background:rgba(173, 239, 239, 0.55)">--最大切片大小</span>
	<span style="background:rgba(240, 167, 216, 0.55)">set mapred.min.split.size.per.node=112345600;</span>
	<span style="background:rgba(240, 167, 216, 0.55)">--每个节点最小切片大小</span>
	<span style="background:rgba(205, 244, 105, 0.55)">set mapred.min.split.size.per.rack=112345600; </span>
	<span style="background:rgba(205, 244, 105, 0.55)">--每个机架的最小切片大小</span>
	set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
	<span style="background:rgba(173, 239, 239, 0.55)">--小文件合并</span>
	当文件比较大的时候，可以增加map数
	set mapreduce.input.fileinputformat.split.maxsize=10485760; --设置切片大小

10. 合理设置reduce数:

	N=min(1009，总输入数据量/256M)

	set mapreduce.job.reduces=N;

11. 开启并行执行：set hive.exec.parallel=true;
12. 使用严格模式
## 4. hive中四种排序的区别
1.<span style="background:rgba(173, 239, 239, 0.55)">order by</span>:全局排序,只有一个reduce参与排序,最终的结果是有序的
2.<span style="background:rgba(240, 167, 216, 0.55)">sort by</span>：在各自的reduce内参与排序,最终的结果是无序的
3.<span style="background:rgba(160, 204, 246, 0.55)">distribute by</span>:按照指定的字段，通过hash算法，把数据发给reduce，在reduce内它是不负责排序的，最终结果是无序的。mod(字段哈希值,reduce的个数)=余数相同的会被划入同一个reduce内
4.<span style="background:rgba(205, 244, 105, 0.55)">cluster by</span>:按照指定的字段，通过hash算法，把数据发给reduce，在reduce内进行升序排序，最终结果是无序的。mod(字段哈希值,reduce的个数)=余数相同的会被划入同一个reduce内

## 5. 分区表和分桶表的区别
a.分区表是<font color="#e36c09">文件夹</font>，分桶是更细粒度的<font color="#e36c09">文件</font>
b.分区表是加快表格<font color="#e36c09">查询速度</font>, 分桶表是加快表连接速度和抽样查询速度
c.分区表的分区字段不能是表中的<font color="#e36c09">现有字段</font>，分桶表的分桶字段是表的现有字段
d.工作中最<font color="#e36c09">常用</font>的是分区表，如果数据量非常大，可以用到分桶表
## 6. 内部表和外部表的区别
a.外部表用<span style="background:rgba(205, 244, 105, 0.55)">external</span>修饰
b.使用<span style="background:rgba(173, 239, 239, 0.55)">truncate</span> 会清空内部表的数据， 对外部表使用会报错
c.使用<span style="background:rgba(240, 167, 216, 0.55)">drop</span>会删除内部表。只会删除外部表的元数据，hdfs上的数据文件不会删除
d.外部表的<span style="background:rgba(255, 183, 139, 0.55)">安全性</span>比较高，所以业务数据一般用外部表存储，临时表用内部表
## 7. 如何查看hive执行计划
--快捷键查看：F5
--sql语句查看：
explain plan for(select * from emp2 where empno = 7788);
select * from table(dbms_xplan.display);
## 8.hive中如何实现拉链表
1. 从基表中筛选出最新的分区数据
2. 从拉链表中筛选出最新的分区数据dt=’9999-12-31’
3. 拉链表和基表做全连接，使用select nvl(基表, 拉链表) ，再加上结束日期date’9999-12-31,’ 当做开链 , 这就就是新用户和老客户最新数据。
4. 拉链表和基表做内连接，select 拉链表原来的字段, 基表的创建时间作为闭链时间 这样就能获取老客户闭链数据
5. 把开链数据和闭链数据使用union all进行拼接，然后覆盖写入到拉链表中。
--选择新用户和老用户的最新数据(开链数据)

```sql
--选择新用户和老用户的最新数据(开链数据)

insert overwrite table  dw_stu_his
select
nvl(ods.id, dw.id) as id,
nvl(ods.sname, dw.sname) as sname,
nvl(ods.tel, dw.tel) as tel,
nvl(ods.create_time, dw.create_time) as create_time,
nvl(ods.create_time, dw.start_time)  as start_time,
'9999-12-31' as end_time,
'9999-12-31' as dt
from
(select * from dw_stu_his  where dt = '9999-12-31') dw--拉链表最新的数据
full join
(select * from ods_stu where dt = '2022-06-15') ods  --基表中最新的数据
on dw.id = ods.id
union all

--选择老用户的闭链数据

select
dw.id,
dw.sname,
dw.tel,
dw.create_time,
dw.start_time,
ods.create_time as end_time,
ods.end_time as dt
from
(select * from dw_stu_his  where dt = '9999-12-31') dw--拉链表最新的数据
join
(select * from ods_stu where dt = '2022-06-15') ods  --基表中最新的数据
on dw.id = ods.id;
```
