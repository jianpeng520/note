--------------------------6.27面试问题（6.30提问）：
## 1.Spark与MR区别
| 比较项    | MapReduce                | Spark                            |
| ------ | ------------------------ | -------------------------------- |
| 数据存储结构 | 磁盘HDFS文件系统               | 使用内存构建弹性分布式数据集RDD对数据进行运算和缓存      |
| 编程范式   | Map+Reduce               | DAG(有向无环图)：Transformation+action |
| 中间结果存储 | 中间结果落地磁盘，IO及序列化反序列化代价比较大 | 中间结果存储在内存中，速度比磁盘多几个数量级           |
| 运行方式   | Task以进程方式维护，任务启动慢        | Task以线程方式维护，任务启动快                |
## 2.spark的组成部分

| 类别            | 技术/组件                       | 描述        |
| ------------- | --------------------------- | --------- |
| **离线分析**      | Spark SQL + DataFrames      | 批处理数据分析   |
| **实时分析**      | Streaming                   | 流数据处理     |
| **机器学习库**     | MLlib                       | 机器学习算法库   |
| **图计算库**      | GraphX                      | 图数据处理与计算  |
| **Spark核心模块** | Spark Core API              | 分布式计算基础框架 |
| **支持语言**      | R, SQL, Python, Scala, Java | 多语言编程支持   |

## 3.spark的角色：
resource manager : 管理集群资源
node manage：管理单个资源
Driver：管理任务的。包工头
Executor：小组长，执行任务的， 管理task线程
task：工人，每个分区就有一个task， task之间是并行运算的
![[Pasted image 20250627215428.png]]
## 4.Spark On Yarn的两种运行模式
   <span style="background:rgba(173, 239, 239, 0.55)">Client模式</span>：客户端模式。 driver运行在Client中， 看日志非常方便， 但是消耗网络性能，适合开发环境
   <span style="background:rgba(240, 167, 216, 0.55)">Cluster</span>：集群模式。 driver运行在Cluster中，看日志非常麻烦， 但是网络性能消耗小， 适合生产环境

## 5.Spark的端口号和版本。 其他软件的版本也要记得(面试细节41问中有)
**端口号**：4041
Oracle 11g
Hadoop 3.2
Spark 3.0
Scala 2.12
Hive hive-3.1.2
Sqoop 1.4.5
Azkaban 2.5
Python 3.7
Linux centos7 / 红帽…
Java 1.8
Mysql 5.7
Datax 3.0
Finereport 11.0
Powerdension 16.5
kettle 7.1
## 6.什么是RDD，RDD的特性有哪些
1. RDD 全称是：Resilient Distributed Dataset (RDD) 弹性分布式数据集
2. RDD 是 Spark 的灵魂，是 Spark 最核心的数据模型
3. Spark 的处理过程就是不同 RDD 之间迭代的过程

Dataset：一个数据集合，用于存放数据的。
Distributed：RDD 中的数据是分布式存储的，可用于分布式计算。
Resilient：RDD 中的数据可以存储在内存中或者磁盘中
![[Pasted image 20250627220742.png]]
## 7.RDD的小文件处理方式
![[Pasted image 20250627221021.png]]
1、需要使用一个新的算子 wholeTextFiles 来解决
2、这个 wholeTextFiles 算子可以将一堆小文件读取到一个 RDD 中，这个 RDD 是有键值对组成，键是文件的路径，值是文件的内容
3、读取之后分区的数量可以手动来设置
`rdd1 = sc.wholeTextFiles('/datas/input/files_100_2',2)`
`#print(rdd1.collect())      #如果文件多或者数据量大，不要使用 collect`
`print(rdd1.take(3))         #输入 RDD 前 3 个元素`
## 8.RDD算子的分类，转换算子和行动算子有什么区别，面试官一定会问你熟悉的有哪些算子
### 转换算子
1. 转换算子都是将一个 RDD 转为另一个 RDD，转换算子的结果一定是 RDD
2. 转换算子调用时不会触发任何任务执行动作
3. 转换算子相当于只搭建了流水线，流式线上没有任何的作业
4. 只有遇到行动算子，才能触发任务执行
### 行动算子
1. 行动算子的结果一定不是 RDD，它的结果一般是列表、保存文件，输出
2. 行动算子基本上都是要去拿到结果
## 9.pyspark如何实现WordCount
`rdd1 = sc.textFile('/lx/1.txt') #['a b c', 'a a c', 'd a a', 'e a c']`
`rdd2 = rdd1.flatMap(lambda x : x.split(' ')) #['a', 'b', 'c', 'a'…]`
`rdd3 = rdd2.map(lambda x : (x,1)) #相当于把 'a' 变成 ('a',1)`
`rdd4 = rdd3.reduceByKey(lambda x, y : x + y) #[('b', 1), ('c', 3),..]`
`print(rdd4.collect())`
## 10.Spark的容错机制(也叫持久化)
     persist缓存机制
     checkpoint检查点机制
### 概述
1. 一般的大数据存储系统为了防止数据丢失一般会采用以下方案：
⚫ 内存快照：将内存中所有数据拍摄一个快照，存储在文件中，读取快照文件恢复内存中数据
⚫ 操作日志：将内存变化操作日志追加记录在一个文件中，下一次读取文件对内存重新操作
⚫ 副本机制：将数据构建多份冗余副本
⚫ 依赖关系：每份数据保留与其他数据之间的一个转换关系

2. Spark 默认就是只记录每一个 RDD 和其他 RDD 之间的血缘关系，数据丢失，就会根据依赖关系重新生成一份即可

### 抛出问题
如果一个 RDD 被后续多个触发算子使用，则每一次调用触发算子都要从头执行，效率较低，所以我们可以将一些复用性较高的RDD 提前进行缓存，以后用到的时候不用从头执行，直接从缓存读取即可。

Spark 提供了 2 中缓存机制：
⚫persist 缓存机制
⚫checkpoint 检查点机制
 ![[Pasted image 20250627221808.png]]
### persist 缓存机制
1、persist 缓存机制可以将 RDD 的数据缓存到内存、硬盘，内存+硬盘
2、persist 缓存机制可以调用两个 API 算子：cache()、persist()
3、cache()就是 persist()，cache 只能将数据缓存到内存
	cache() ====> persist（StorageLevel.MEMORY_ONLY）
4、persist 的缓存是一次性的，只能在一个代码中起作用，当你下一次在此运行代码时，需要重新构建缓存
5、persist 会保留血缘关系
### 缓存的级别
#### 缓存在本地磁盘中：注意这不是 HDFS
StorageLevel(磁盘存储, 内存储存, 堆外内存,序列化形式存储,副本数量)

堆外内存：在 Java 应用程序中，对象默认是在 JVM 的堆内存中分配的，这部分内存由 JVM 管理，并且会受到垃圾回收机制的影响。而堆外内存则是直接分配在操作系统级别的内存中，这可以减少垃圾回收的压力，不经过 JVM 的管理。

序列化形式存储：数据将以序列化的格式存储。通常会占用更少的空间，但是读取时需要反序列化，可能会增加一些 CPU 开销。

StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)

StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)

StorageLevel.DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)

#### 缓存在内存中

StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)

StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)

##### 缓存在内存中，如果内存不足就写入磁盘

StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)

StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)
#### 缓存在堆外内存中：不是 Executor 内存
StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)

```python
import time
from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext, StorageLevel
rdd4 = sc.textFile('/lx/a/a1.txt')
rdd5 = rdd4.flatMap(lambda x: x.split(' '))
rdd6 = rdd5.map(lambda x: (x, 1))

# --------------在此添加 persist 缓存----------------

rdd6.persist(StorageLevel.MEMORY_AND_DISK)
rdd7 = rdd6.reduceByKey(lambda x, y: x + y)
rdd8 = rdd6.map(lambda x:(x[0],x[1] * 10))
print(rdd7.collect())
print(rdd8.collect())

#--------------在此释放 persist 缓存----------------

rdd6.unpersist(blocking=True)
time.sleep(1000)
sc.stop()
```
![[Pasted image 20250627222404.png]]
### checkpoint 检查点机制
1、之前 Persist 机制存储的 RDD 缓存数据都是本地磁盘和内存中，还是会出现丢失问题
2、为了彻底解决丢失焦虑问题，Spark 提供了 CheckPoint 机制，将 RDD 数据保存到 HDFS 上，可以理论上保证缓存数据永远不丢失
3、CheckPoint 的缓存，只要存一次以后，以后每次运行代码，缓存都可以重复使用
4、CheckPoint 不会保留血缘关系

```python
import time
from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext, StorageLevel
if __name__ == '__main__':
	spark =
	SparkSession.builder.appName('c').master('local[*]').getOrCreate()
	sc = spark.sparkContext
	rdd4 = sc.textFile('/lx/a/a1.txt')
	rdd5 = rdd4.flatMap(lambda x: x.split(' '))
	rdd6 = rdd5.map(lambda x: (x, 1))

	#--------------在此添加 checkpoin 缓存----------------

	#1、指定一个 HDFS 的缓存路径

	#2、将 RDD 添加到缓存

	sc.setCheckpointDir('hdfs://localhost:8020/checkpoint')
	rdd6.checkpoint()
	rdd7 = rdd6.reduceByKey(lambda x, y: x + y)
	rdd8 = rdd6.map(lambda x:(x[0],x[1] * 10))
	print(rdd7.collect())
	print(rdd8.collect())
	time.sleep(1000)
```
![[Pasted image 20250627222704.png]]
### persist 和 checkpoint 区别
![[Pasted image 20250627222728.png]]

## 11.spark共享变量2种方式
    广播变量：让每个task共享变量
    累加器：Executor中变量的结果会反馈给Driver中的变量进行累加
### Spark 广播变量
1、在 Spark 程序中，RDD 是存放在不同主机的 Executor 内存中，是分布式存储，而普通的变量是在 Driver所在主机的内存中进行存储。

2、如果你要将 RDD 和普通变量进行联合操作，每当你调用一次行为算子，都会从 Driver 把普通变量拷贝一次到 Executor，如果普通变量数据量较大，每次都要消耗大量网络的网络带宽，影响计算效率。
解决方法是：提前将普通变量通过广播的方式发送给每一个 Executor，当以后计算时，Spark 可以从本地
Executor 内存读取这个变量 rdd=[1,2,3,4,5,6]
![[Pasted image 20250627223325.png]]

```python
#不使用广播变量
val=10
rdd1=sc.parallelize([1,2,3,4,5])
rdd2=rdd1.map(lambda x:x+val).collect()
print(rdd2)
#使用广播变量
val2=sc.broadcast(10)
rdd3=rdd1.map(lambda x:x+val2.value).collect()
print(rdd3)
#广播变量会在每个 Executo 上保存一份副本，当不需要时，可以暂时删除副本，释放内存资源
val2.unpersist()
```
#### 广播变量在以下情况下非常有用：
1. 当有一个大型数据集或对象需要在集群节点之间共享时，以避免在每个节点上复制数据（broadcast 存储在集群中每个节点的内存中，确保分配了足够的内存来存储它）。
2. 当数据是只读的，不需要修改时。
#### 不应该使用广播变量的情况包括：
1. 当数据集较小，可以容易地复制到每个节点时，广播变量可能会引入不必要的开销。
2. 当需要在节点之间共享可变状态时，广播变量不适用。
### Spark 累加器
1、一个普通的变量如果被多个 Executor 线程来进行累加，这个普通变量会被拷贝多个副本到线程中参与运算，运算之后并不会改变普通变量的值，因为普通变量在 Driver 中

2、为了解决这个问题，我们引入累加器，累加器可以强制各个 Executor 线程对变量累加之后要对 Driver 中的变量进行反馈累加

#### 没有定义累加器
```python
#1、定义列表
rdd1 = sc.parallelize([1,2,3,4,5,6], 3)
print(rdd1.glom().collect()) #[[1, 2], [3, 4], [5, 6]]
#自己写代码，统计 rdd1 中元素的个数
sum = 0
def my_func(x):
global sum #声明为全局变量
sum += 1
rdd1.foreach(lambda x : my_func(x))
#打印
print(sum) #结果为 0
```
#### 使用累加器
```python
#1、定义列表
rdd1 = sc.parallelize([1,2,3,4,5,6], 3)
print(rdd1.glom().collect()) #[[1, 2], [3, 4], [5, 6]]
sum = sc.accumulator(0) #定义累加器,初始化为 0
def my_func(x):
sum.add(1) #在使用累加器
rdd1.foreach(lambda x : my_func(x))
print(sum) #结果为 6
```
![[Pasted image 20250627223824.png]]

## 12.Spark宽窄依赖
	 窄依赖：父RDD和子RDD分区之间是一对一的关系，不会引起Shuffle
	 宽依赖：父RDD和子RDD分区之间是一对多的关系，会引起Shuffle
### 窄依赖
父 RDD 的一个分区的数据只给了子 RDD 的一个分区【不用调用分区器】
#### 通俗的判断方式
看父 RDD 的分叉，如果全部的分区都只有一个分叉，那么一定是窄依赖
#### 会引起窄依赖的算子
map，filter，union
#### 面试回答的方式
父 RDD 和子 RDD 分区之间是一对一的关系
### 宽依赖
父 RDD 的一个分区的数据给了 RDD 的多个分区【需要调用 Shuffle 的分区器来实现】
宽依赖一般会有 Shuffle
#### 通俗的判断方式
看父 RDD 的分叉，如果父 RDD 全部的分区有可能有多个分叉，那么一定是宽依赖
#### 会引起宽依赖的算子
reduceBykey、groupByKey、distinct、join
#### 面试回答的方式
父 RDD 和子 RDD 分区之间是一对多的关系
### 你对宽窄依赖的理解
1、宽窄依赖描述的是父 RDD 和子 RDD 之间的依赖关系
2、窄依赖：父 RDD 和子 RDD 分区之间是一对一的关系，不会引起 Shuffle
3、宽依赖：父 RDD 和子 RDD 分区之间是一对多的关系，会引起 Shuffle
### Spark 的内存迭代过程
1、DAG 基于宽依赖(Shuflle)被划分为多个 State（阶段）
2、每个 Stage 阶段内部为窄依赖，可以组成 PipLine
3、一个 PipLine 可以由一个 Task 所处理（一个线程），即每条 PipLine 的处理均是各自的线程处理，即内存中完成了 PipLine 的迭代
4、当第一个 stage 结束时，该 stage 的线程将会被释放，下一个 stage 重新开辟新的线程来处理
![[Pasted image 20250628145142.png]]
![[Pasted image 20250628145206.png]]

### Spark 的 Shuffle
1、Spark 的 Shuffle 借鉴了 MapReduce 的 Shuffle
2、Spark 的 Shuffle 过程是：上一个 Stage 的线程将处理的结果保存到磁盘上，由下一个 Stage 的线程来读取
3、由于 Spark 的 Shuffle 过程有磁盘 IO 操作，会影响 Spark 的处理速度
## 13. Spark中的Job、Stage、Task的概念
### Job：
一个 action 算子就会触发一个 Job。
### Stage：
一个 Job 分为一个或者多个 Stage，Stage 以 RDD 宽依赖为界，shuffle 前后的 RDD 属于不同的 Stage
### Task：
一个 Stage 包含一个或者多个 Task，一个 Stage 包含的 Task 的数量等于这个 Stage 最后一个 RDD 的
partition 的数量。Task 是 Executor 执行任务的最小单位
## 14.spark sql的执行过程
### 1.解析阶段: 
Spark 使用 Catalyst 引擎将用户输入的 SQL 语句解析为抽象语法树（AST）,生成初始逻辑计划 LogicalPlan
### 2.分析阶段: 
通过元数据和表的 Schema 校验逻辑计划中的字段、函数等，并为计划补充缺失信息，生成一个经过校验和补全的逻辑计划 Analyzed Logical Plan
### 3.逻辑优化阶段：
对逻辑计划进行规则化优化，比如谓词下推、列剪裁等，一个经过优化的逻辑计划Optimized Logical Plan
### 4.物理计划生成阶段：
将逻辑计划转化为物理计划，选择最优执行方案划作为 Physical Plan
### 5.代码生成阶段：
对物理计划中的部分操作生成更高效的 Java 字节码，输出带有代码信息的物理计划
### 6.RDD 生成阶段：
将物理计划转化为低层次的 RDD 操作，生成 Spark 执行引擎中直接运行的 RDD DAG
### 7.执行阶段：
提交作业并执行 RDD 转换
## 15.有余力的同学记几个   Spark常见的错误
### container 因内存不足被 yarn kil
JavaScriptExecutorLostFailure (executor 374 exited caused by one of the running tasks) Reason:

Container killed by YARN for exceeding memory limits

#### 解决办法
1. 增大：spark.yarn.executor.memoryOverhead ##确保每个 Executor 有足够的堆外内存
2. 或者扩大并发：spark.sql.shuffle.partitions（默认 200）；
3. 在开启 AE(spark.sql.adaptive.enabled=true)后，最大 shuffle tasks 数由spark.sql.adaptive.maxNumPostShufflePartitions  ##启用自适应执行, 让 Spark 根据实际运行情况动态调整 Shuffle 分区数量
### Shuffle Fetch Failed
```python
JavaScriptCaused by: org.apache.spark.SparkException: Job aborted due to stage failure:

ShuffleMapStage 11 (run at ThreadPoolExecutor.java:1142) has failed the maximum allowable

number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException:

Connection from n20-215-213.byted.org/10.20.215.213:7337 closed at

org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockF

etcherIterator.scala:569)
```
#### 解决办法：
##### 设置 hdfs-base-shuffle:
```python
spark.shuffle.hdfs.enabled=true # Shuffle 数据写入 HDFS

spark.shuffle.io.maxRetries=1 #指定 Shuffle fetch 失败后重试的最大次数

spark.shuffle.io.retryWait=0s #指定每次重试之间的等待时间

spark.network.timeout=120s #设置网络通信的超时时间
```

### 拉取 frame 太大
HTTPorg.apache.spark.shuffle.FetchFailedException: Too large frame: 2226905783

通常是因为 Shuffle 过程中某个分区的数据块过大，导致无法通过网络传输或在内存中处理

#### 解决方法：
`set spark.maxRemoteBlockSizeFetchToMem=536870912;  #控制了从远程节点获取的最大数据块大小`
### 获取太多 hive 分区

NginxCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Fetch

to many partitions 20939 max: 12000)

#### 解决方法：
1. 检查 SQL 是否正确，是否真的读取太多分区
2. spark 设置如下参数：
spark.sql.hive.convertMetastoreParquet=true; # Spark 会直接读取 Hive 元存储中的 Parquet 表，而不需要通过 Hive SerDe 接口

spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER; # Spark 将完全依赖 Hive 元存储中的列名定义，而不进行任何推断或修改。这可以确保列名的一致性和稳定性
### 堆外内存不足

JavaScriptCaused by: org.apache.spark.SparkException: Job aborted due to stage failure:

ShuffleMapStage 4 (run at ThreadPoolExecutor.java:1142) has failed the maximum allowable number

of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: failed

to allocate 16777216 byte(s) of direct memory (used: 4294967296, max: 4294967296)

#### 1. 修改堆外内存值
spark.executor.extraJavaOptions=-XX:MaxDirectMemorySize=2560m
#### 2. 调整并发：
spark.sql.shuffle.partitions（默认 200）
## 16.spark sql如何看执行计划
### 1.执行计划几种模式
<span style="background:rgba(173, 239, 239, 0.55)">explain()</span>：只展示物理执行计划。（使用较多）
<span style="background:rgba(240, 167, 216, 0.55)">explain(mode="simple")</span>：只展示物理执行计划。
<span style="background:rgba(160, 204, 246, 0.55)">explain(mode="extended")</span>：展示物理执行计划和逻辑执行计划。
<span style="background:rgba(205, 244, 105, 0.55)">explain(mode="codegen") </span>：展示要 Codegen 生成的可执行 Java 代码。
<span style="background:rgba(255, 183, 139, 0.55)">explain(mode="cost")</span>：展示优化后的逻辑执行计划以及相关的统计。
<span style="background:#fff88f">explain(mode="formatted")</span>：以分隔的方式输出，它会输出更易读的物理执行计划，并展示每个节点的详细信息。
### 2.使用方法
spark.sql("select empno, sal from bigdata.emp where deptno = 10").explain(mode="extended")

explain extended select empno, sal from bigdata.emp where deptno = 10;

### 3.执行计划中几个关键字
<span style="background:rgba(173, 239, 239, 0.55)">FileScan</span>：表扫描
<span style="background:rgba(240, 167, 216, 0.55)">Filter</span>：过滤
<span style="background:rgba(160, 204, 246, 0.55)">Project</span>：列裁剪优化，选取需要的字段
<span style="background:rgba(205, 244, 105, 0.55)">HashAggregate</span>：聚合计算，如果在 map 端表示 map 预聚合
<span style="background:rgba(255, 183, 139, 0.55)">Exchange hashpartitioning</span>：shuflle 混洗，hash 分区
<span style="background:rgba(3, 135, 102, 0.2)">HashAggregate</span>：聚合计算
<span style="background:#affad1">Sort</span>：排序
<span style="background:#d3f8b6">SortMergeJoin</span>：join 关联
<span style="background:#fff88f">Window</span>：开窗函数计算
<span style="background:rgba(240, 200, 0, 0.2)">BroadcastExchange</span>：小表广播
<span style="background:rgba(240, 107, 5, 0.2)">BroadcastHashJoin</span>：广播 join
### 4.执行计划解析
```sql
explain
select job, count(distinct empno) as cnt
from bigdata.emp
where deptno = 20
group by job;
```
![[Pasted image 20250628150932.png]]
1.<span style="background:rgba(173, 239, 239, 0.55)">Scan</span>:扫描表
2.<span style="background:rgba(240, 167, 216, 0.55)">Filter</span>:过滤条件 WHERE deptno = 20，只保留 deptno 等于 20 的记录
3.<span style="background:rgba(160, 204, 246, 0.55)">Project</span>:选择需要的列 empno 和 job
4.<span style="background:rgba(205, 244, 105, 0.55)">HashAggregate</span>:基于 job 和 empno 两个字段进行分组，目的是去除重复的 job 和 empno 组合。这一步确保每个 job 和 empno 的组合只出现一次
5.<span style="background:rgba(255, 183, 139, 0.55)">Exchange</span>:根据 job 和 empno 进行哈希分区，确保相同 job 和 empno 组合的数据被分配到相同的分区中
6.<span style="background:#d3f8b6">HashAggregate</span>:类似于第 4 步，这一步是为了进一步确保数据的唯一性，去除重复的 job 和 empno 组合
7.<span style="background:#b1ffff">HashAggregate</span>:在每个分区内部基于 job 进行局部聚合，计算每个 job 下不同 empno 的部分计数，以减少全局聚合的数据量。
8.<span style="background:#fff88f">Exchange</span>:数据根据 job 字段进行哈希分区，确保相同 job 的数据被分配到相同的分区中，为最终的全局聚合操作做准备。
9.<span style="background:rgba(140, 140, 140, 0.12)">HashAggregate</span>:基于 job 进行分组，并计算每个 job 下不同的 empno 数量
### 5.开窗函数执行计划
![[Pasted image 20250628151145.png]]

1. <span style="background:rgba(173, 239, 239, 0.55)">Scan 扫描表</span>：只选择必要的列，empno, ename, sal, deptno, job
2. <span style="background:rgba(240, 167, 216, 0.55)">Filter</span>：过滤条件 WHERE deptno > 10，只保留 deptno 大于 10 的记录
3. <span style="background:rgba(160, 204, 246, 0.55)">Exchange 初次分区</span>：根据 deptno 进行哈希分区，确保相同 deptno 的数据被分配到相同的分区中，为后续的窗口函数计算做准备
4. <span style="background:rgba(205, 244, 105, 0.55)">Sort 排序</span>：对数据按 deptno 字段进行排序，确保在每个分区内的数据是有序的
5. <span style="background:rgba(255, 183, 139, 0.55)">Window 窗口函数计算</span>：在每个 deptno 分区内计算最大工资和最小工资
6. <span style="background:#fdbfff">Project 选择列</span>：选择需要的列：empno, ename, sal, job, max_sal, min_sal
7. <span style="background:#b1ffff">Exchange 二次分区</span>：根据 job 字段进行哈希分区，确保相同 job 的数据被分配到相同的分区中
8. <span style="background:#affad1">Sort 排序</span>：对数据按 job 字段进行排序，确保在每个分区内的数据是有序的
9. <span style="background:#d3f8b6">Window 窗口函数计算</span>：在每个 job 分区内计算记录数 COUNT(1)
10. <span style="background:#fff88f">Project 选择列</span>：最终选择所有需要的列：empno, ename, sal, max_sal, min_sal, cnt
## 17.spark sql优化 和 spark sql数据倾斜
