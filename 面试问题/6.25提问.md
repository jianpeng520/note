## 1. 说说你在oracle中用存储过程做了哪些事情
建立日志表，方便程序在运行过程中查找、分析错误
批量对源表至目标表的数据插入，包含计算派生指标
拉链表的更新

## 2. 一个 plsql(存储过程)脚本很慢，如何分析到底哪里慢
通过<span style="background:#40a9ff">日志</span>，记录每步耗时，分析到底哪一步慢，
然后找到最慢的 sql 部分，看到底是查询慢还是更新慢，
如果查询慢，看 select 后面是否调用了其他的<span style="background:#40a9ff">自定义函数</span>，注释函数，再查性能是否提升，如果调了自定义函数之后变慢，则需要优化自定义函数的性能
如果仍然很慢，这个时候就要分析<span style="background:#40a9ff">数据量的变化</span>和<span style="background:#40a9ff">执行计划</span>，主要看数据量的变化情况，
是否<span style="background:#40a9ff">突然数据增量比较快</span>，导致数据量暴增；还可以通过日志看，脚本执行的<span style="background:#40a9ff">效率是否越来越慢</span>，又分为数据量变得越来越大 或者 有人改过代码。

数量多了，之前同步的方式可能需要更改，例如全量改增量

如果有人为了优化性能，建了更多的索引导致 插入数据越来越慢， 这时候需要平衡查询和更细的性能，有可能需要增加服务器的配置
## 3. linux中你最熟悉哪些命令
1.查看cpu ：cat /proc/cpuinfo
2.查看内存：cat /proc/meminfo
  适合人看的方式： free -h
3.查看硬盘：df -h
4.查看ip地址：ip addr
5.查看网络是否通顺： ping  www.baidu.com
6.查看端口
  a.下载网络工具：yum -y install net-tools
  b.查看所有端口：netstat -aln
7.查看22端口是否被占用 ： netstat -aln | grep 22
8.查看进程：ps -aux    
			top
9.杀进程：kill     pid号(进程id)
## 4.说下你用shell在项目中干了什么事情
用shell脚本封装操作hive任务调度，配合AZkaban实现一步到位抽取数据
## 5.kettle如何优化

```
a.修改spoon.bat文件

把if "%PENTAHO_DI_JAVA_OPTIONS%"=="" set PENTAHO_DI_JAVA_OPTIONS="-Xms1024m" "-Xmx2048m"  中 -Xms和-Xmx的值调大

Xms ： 是jvm启动时，内存初始值大小

Xmx ： 是jvm运行时，可申请的最大内存

**b.表输出组件中，可以把*提交记录数量*值调大*
```
## 6.kettle，datax，sqoop有什么区别（每天背诵知识点）
|        |                |                   |                       |
| ------ | -------------- | ----------------- | --------------------- |
| **工具** | **最低速度 (条/秒)** | **最高速度 (条/秒)**    | **典型场景说明**            |
| Sqoop  | 5,000 - 10,000 | 500,000 - 1M+     | 基于Hadoop MR，并行度高时性能极佳 |
| DataX  | 3,000 - 8,000  | 200,000 - 500,000 | 单机性能优秀，依赖通道数配置        |
| Kettle | 1,000 - 5,000  | 50,000 - 100,000  | 图形化工具，适合中小规模数据        |

|          |                                                                                                              |                                                                                    |                                                                                                 |
| -------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **对比维度** | **Sqoop**                                                                                                    | **DataX**                                                                          | **Kettle (Pentaho DI)**                                                                         |
| **核心定位** | 专为Hadoop设计的批量数据迁移工具                                                                                          | 阿里开源的高性能异构数据同步工具                                                                   | 企业级ETL工具，支持复杂数据处理流程                                                                             |
| **优点**   | ✅ 原生Hadoop生态支持（HDFS/Hive/HBase）  <br>✅ 并行度高，性能极佳（基于MapReduce）  <br>✅ 自动类型映射（如MySQL→Hive）  <br>✅ 命令行操作，适合调度集成 | ✅ 多数据源支持（RDBMS/NoSQL/文件等）  <br>✅ 单机多线程，性能稳定  <br>✅ JSON配置化，易于版本管理  <br>✅ 无依赖，轻量级部署 | ✅ 图形化界面（Spoon），开发友好  <br>✅ 支持复杂转换（聚合、过滤、Join等）  <br>✅ 社区插件丰富（如HTTP请求、XML解析）  <br>✅ 支持作业流调度和依赖控制 |
| **缺点**   | ❌ 仅支持Hadoop与关系型数据库间同步  <br>❌ 实时同步能力弱（依赖定时任务）  <br>❌ 需Hadoop集群环境，资源消耗大                                        | ❌ 无图形化界面，调试复杂  <br>❌ 单机架构，扩展性受限（需自行分片）  <br>❌ 复杂转换需编写插件（Python/Java）               | ❌ 性能较低（受JVM限制，需调优）  <br>❌ 大数据量处理易内存溢出  <br>❌ 集群部署需商业版（如Pentaho）  <br>❌ 学习成本高（复杂转换设计）            |
## 7.finereport连接的数据源是数据库还是文件。 finereport的服务端和客户端分别在什么环境下
- **数据库**：适用于企业级报表，数据量大、实时性要求高。
- **文件**：适用于临时分析或小型数据报表。

| 组件  | 环境                       | 作用             |
| --- | ------------------------ | -------------- |
| 服务端 | Linux                    | 报表生成、权限管理、数据查询 |
| 客户端 | 浏览器（Chrome/Firefox/Edge） | 查看/交互式分析报表     |

## 8.hive中如何实现行转列和列转行（每天背诵知识点）
### 行转列
```sql
--方法1：
select
y,
sum(case  when  q = 'Q1' then amt else 0  end) as q1,
sum(case  when  q = 'Q2' then amt else 0  end) as q2,
sum(case  when  q = 'Q3' then amt else 0  end) as q3,
sum(case  when  q = 'Q4' then amt else 0  end) as q4
from amount
group by y;

--方法2：

select
y,
sum(if(q = 'Q1', amt , 0)) as q1,
sum(if(q = 'Q2', amt , 0)) as q2,
sum(if(q = 'Q3', amt , 0)) as q3,
sum(if(q = 'Q4', amt , 0)) as q4
from amount
group by y;

--方法3：

select
y,
collect_list(amt)[0] as q1,
collect_list(amt)[1] as q2,
collect_list(amt)[2] as q3,
collect_list(amt)[3] as q4
from(select * from amount order by y, q) t
group by y;
```
### 列转行
```sql
--方法1：
select y, 'Q1' as q, q1 as amt from amount2
union all
select y, 'Q2',      q2        from amount2
union all
select y, 'Q3',      q3        from amount2
union all
select y, 'Q4',      q4        from amount2;

--方法2：

select y, q, amt from amount2
lateral view explode(map('Q1', q1,'Q2', q2,'Q3', q3,'Q4', q4)) v as q, amt;

```
## 9.oralce中如何实现行转列和列转行（每天背诵知识点）
### --case when行转列写法
```sql
select deptno, sal,
       case when deptno = 10 then sal else 0 end  as  "10",
       case when deptno = 20 then sal else 0 end  as  "20",
       case when deptno = 30 then sal else 0 end  as  "30"
  from emp;
```
### --pivot实现行转列
```
select *  from (select deptno, sal from emp)
 pivot (sum(sal) for deptno in (10,20,30));
```
## 10.hive和oracle如何找到连续三天都登入的用户（每天背诵知识点）
### oracle
```sql
select uname from

(
select uname, dt ,
       dt - row_number()over(partition by uname order by dt) as r
  from u
)
group by uname, r

having count(1) >= 3;
```
### hive
```sql
select name from
(
select 
name, dt, 
date_add(dt, -row_number()over(partition by name order by dt)) as r
from u
)t
group by name, r
having count(1) >= 3;
```
