## 1.ETL过程中，如何保证顺准确性和完整性
<p align="left"></p>在ETL 过程中，常见的错误类型包括数据类型不匹配、数据丢失和数据重复等。
### 1、数据类型不匹配
数据类型不匹配是指源数据和目标数据表之间的数据类型不一致。例如，在数据抽取过程中，源数据中的一个字段为字符串类型，而目标数据表中对应字段为整数类型，这样的数据类型不匹配可能导致数据转换错误或丢失精度。为应对这种情况，ETL 过程中应该有严格的数据类型映射规则，并在数据加载前进行数据类型转换或者数据校验，确保数据类型的一致性。
有些公司为了避免出现数据类型不一致的情况，把 ods 的所有表的数据类型都改为 string 格式现在抽数据不报长度问题，以后可能会报，所以建表的时候，类型长度可以比数据字典的稍微大些。
### 2、数据丢失
数据丢失是指源数据中的部分记录在 ETL 过程中未能正确抽取或加载到目标数据表中。
可能的原因包括源数据源头数据缺失、ETL 过程中的筛选条件问题或数据加载失败等。
为防止数据丢失，应该建立全面的数据源监控机制，及时发现源数据变更或数据源异常，确保数据采集的完整性。
脚本未配置正确的依赖关系
### 3、数据重复
数据重复是指在目标数据表中出现重复的记录。可能是由于 ETL 过程中未对重复数据进行去重操作或者数据加载规则不严谨导致的。数据重复会浪费存储空间，并在数据分析时引入偏差。为避免数据重复，ETL过程应该设计合理的去重策略，并在数据加载前进行重复数据的检查和处理。
Etl 未设置类似于 truncate 这种属性，多次抽取，导致数据重复
### 数据校验方法：
统计源表和目标的数据量，看是否一致;抽样取源表和目标表部分数据进行比对。
两张表的关键字段做 minus（oracle 或者 Gauss）或者 left join(通用)计算 (适合于同一个数据库)
如果是 hive 中，因为 hive 没有主键约束，可以通过 group by 分组 having>=2 过滤，看是否有重复数据。

## 2.如何保证你的指标是正确的
1. 与业务反复沟通需求，保证理解的需求与业务理解的是同一个意思
2. 和业务核对计算口径(取数逻辑)数据探源的时候，保证探源的表是正确的
3. 写的 sql 逻辑是正确的
4. 保证上游的数据和依赖关系没有什么问题
- 单元测试：我会先按照计算公式进行手工计算，然后通过代码再算一遍，对比两次计算结果
- 代码评审：可以进一步确认我们对需求的理解是否正确
- 总分校验：通过把总账数据与账务明细数据按科目进行余额核对（可以检查出会计登记数据与实际业务发生数据是否一致）
- 业务也会在业务系统中(如 SAS 系统)查看我们的开发结果是否与业务系统运行的结果一致

自有数据平台监控：通过设置指标阈值波动范围，超出这个范围，立马告警
## 3.你们每天的数据量有多大

## 4.你们跑批时间是什么时候，谁负责跑批调度，跑批时间多久
是你自己写的 sql，1 千万的数据要计算多久，可以说 1-10 分钟左右
跑批时间是和数据量，sql 的复杂程度，优化等等相关的。
Azkaban 这种调度工具全流程跑批（运维工程师的或）：
5-6 千万的数据：1-2 个小时左右
1 亿多：跑 2-4 个小时左右
月批数据：跑二三十个小时

## 5.你们一般什么时候上线，是谁负责上线的
每周三或者周五上线晚上的 8 点上线(业务,开发，运维加班）
## 6.数据测试的时候， 你们的测试数据从哪来
python自己造，navicat,买的，共享
## 7.是你对接的需求的吗，需求文档是什么样子的
word,
## 8.接到需求后，你是怎么开展工作的讲一个你最熟悉的指标，说说从0到1你是怎么开发的
1. 拿到需求后，会认真看需求文档，查看需求文档中有哪些需求
2. 和业务多次确认需求和计算口径，保证和业务理解的没有差别
3. 假如要计算一个指标 ：最近一周各机构各贷款产品的的贷款总额：
	a) 这是一个派生指标，首先我们对该指标进行拆解
	b) 拆解出原子指标：贷款总额
	c) 拆解出维度：时间维度(周)，机构维度(支行或分行),产品维度(不用产品)
4. 对数据进行探源，找到维度，原子指标所对应的上游表格(自己找出表格的名字)
5. 数据探源以后，确认哪些是维度表，哪些是事实表，他们之间的关联字段是什么
6. 采用维度建模-星型模型的方式来梳理他们之间的关系
7. 写 sql 开发这个指标，并优化这些 sql，保证 sql 的执行效率
8. 对指标进行测试，保证指标的准确性